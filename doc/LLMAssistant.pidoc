\documentclass PIScriptDoc

\script LLMAssistant

\keywords {
   LLM, AI, integration
}

\author {
   Scott Stirling, scott@stirlingastrophoto.com
}

\copyright {
   2025
}

\brief {
   LLM Assistant for PixInsight integrates chat between a local or cloud-based Large Language Model (LLM) and the PixInsight workspace.
}

\description {
   LLM Assistant for PixInsight integrates chat between a local or cloud-based Large Language Model (LLM) and the PixInsight workspace. It acts as an astrophotography processing assistant, aware of a selected image's data and metadata, to provide advice on next processing steps, help understand the image data, and generate descriptions of finished work.
}

\parameter {  LLM URL } {
  Enter the full URL of an LLM's chat completions API endpoint.

    \list {
        {For LM Studio, the default is http://127.0.0.1:1234/v1/chat/completions}

        {For Ollama, the default is http://127.0.0.1:11434/v1/chat/completions}

        {For a Cloudflare AI Gateway, it will look like https://gateway.ai.cloudflare.com/v1/${ACCOUNT_ID_STRING}/${API_GATEWAY}/workers-ai/${MODEL_PATH} where the model is specified in the URL.}

        {For a Google AI API, the URL will look like https://generativelanguage.googleapis.com/v1beta/openai/chat/completions and the model is specified as a separate configuration value.}
    }
}
\parameter {  API Key } {
    For local servers, you can typically leave the default "no-key".

    For cloud services, enter your API token for your account's authentication.
}
\parameter {  Model } {
    This field is often required by cloud services to specify which model to use, though some vendors put the model name in the URL. It can be left blank for local LLM servers.

    For a Cloudflare AI Gateway, an example might be @cf/meta/llama-4-scout-17b-16e-instruct.

    For Google AI, an example might be gemini-2.0-flash.

    For local servers like llama.cpp and LMStudio where one chat model is running on the default port, this field can be left blank}
\parameter {  Temperature } {
  Controls the "creativity" and randomness of the LLM's responses. The default is a good starting point and anywhere from 0.8 to 1.2 is normal.
}
\parameter {  Max Tokens } {
  imits the length of the LLM's responses. The maximum tokens supported vary by LLM model and vendor. Chat history counts toward the max token count.
}
\parameter { Enable Visual Analysis } {
  Option to enable or disable sending image data to the LLM. Default is disabled.
}
\parameter { Vision max pixels } {
  Set to the maximum supported by the visual LLM, which is referenced if needed to resize the LLM's copy of a selected image. The maximum supported varies by vendor and model. See your vendor's documentation, but safe bets are 1024 for local models and 2048 for remote vendor APIs.
}
\parameter { System Prompt } {
   A default system prompt is provided and can be customized to change the assistant's behavior.
}


\usage {
  Once configured, you may begin chatting to the LLM directly through the input text area, using Ctrl+Enter as a keyboard shortcut to Send, or use the Send button.

Open one or more images in your PixInsight workspace. For best results, use images that have been plate-solved with astrometric data and have been saved with processing history and/or XISF or FITS headers.

Go to Script > Utilities > LLM Assistant to launch the main tool.

The chat window will appear. See the Configuration section ^ if needed.

Select a Target Image:
    Use the dropdown menu at the top left of the window to choose an open image to work on.

Analyze:
    Click the "Analyze Selected Image" button. The script will gather details about the image and its processing history and send the details to the LLM and, if opted in, a copy of the image as a JPG is sent after being resized to fit the configured maximum dimensions for the LLM API.

Chat The first response from the LLM will appear. You can now have a conversation:
    \list {
        {Ask for recommendations: "What should I do next?"}
        {Ask for clarification: "Explain what DynamicBackgroundExtraction does."}
        {Ask for a description: "Please write a description for this image for AstroBin."}
    }
}

\make
